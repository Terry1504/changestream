changestream {
  mysql {
    host = "localhost"
    port = 3306
    user = "root"
    password = "password"

    // Changestream must have a server-id that is unique in the cluster
    server-id = 93

    // Timeout for binlog client and column info queries
    timeout = 5000 #ms

    // Keepalive interval for binlog client
    keepalive = 5000 #ms

    // Preload settings for the column info actor
    preload {
      // Preload specific databases (optional)
      databases = "" #comma separated

      // The timeout for the column info query should be longer than other timeouts,
      // since we are querying the INFORMATION_SCHEMA
      timeout = 30000 #ms
    }

    host = ${?MYSQL_HOST}
    port = ${?MYSQL_PORT}
    user = ${?MYSQL_USER}
    password = ${?MYSQL_PASS}
    server-id = ${?MYSQL_SERVER_ID}
    timeout = ${?MYSQL_TIMEOUT}
    keepalive = ${?MYSQL_KEEPALIVE}
    preload.databases = ${?MYSQL_PRELOAD_DATABASES}
    preload.timeout = ${?MYSQL_PRELOAD_TIMEOUT}
  }

  // Fully qualified class path of the actor to use for emitting events
  // Default:
  //   emitter = "changestream.actors.StdoutActor"
  emitter = ${?EMITTER}

  // Include any database or table identifiers that you'd like to include.
  // This is an explicit list so no other db/table combinations will be included.
  // For example:
  //   whitelist = "my_app.*,my_other_app.customers"
  // Or provide them as ENVs
  whitelist = ${?WHITELIST}

  // Exclude database or table identifiers. The whitelist supercedes the blacklist setting.
  // Note, the following databases are always ignored:
  //   - information_schema
  //   - mysql
  //   - performance_schema
  //   - sys
  //
  // For example:
  //   blacklist = "my_other_app.transactions,my_other_app.other_stuff"
  // Or provide them as ENVs
  blacklist = ${?BLACKLIST}

  // Whether or not to include the row data
  include-data = true

  // To create a key (scala console):
  //   val keygen = javax.crypto.KeyGenerator.getInstance("AES")
  //   keygen.init(128) // note doesn't work in 256 yet
  //   java.util.Base64.getEncoder.encodeToString(keygen.generateKey().getEncoded)
  encryptor {
    enabled = false
    encrypt-fields = "row_data,old_row_data,query.sql"
    timeout = 500 #ms
    cipher = "AES"
    key = "yRVCuDlryZdWhSUDwLNugQ=="

    enabled = ${?ENCRYPTOR_ENABLED}
    encrypt-fields = ${?ENCRYPTOR_ENCRYPT_FIELDS}
    timeout = ${?ENCRYPTOR_TIMEOUT}
    cipher = ${?ENCRYPTOR_CIPHER}
    key = ${?ENCRYPTOR_KEY}
  }

  aws {
    region = "us-east-1"
    sqs {
      queue = "firehose"
    }

    sns {
      // Optionally interpolate database name and/or table name in topic, for example:
      //   "{database}-firehose"
      //   "{tableName}-firehose"
      //   "{database}.{tableName}-firehose"
      // Note: String interpolation is only supported in SNS at this time
      topic = "firehose"
    }

    s3 {
      // The bucket name that is the destination for change events
      bucket = ""
      // Key prefix to use (e.g. production_db/)
      key-prefix = ""
      // Maximum number of changes to buffer before writing to S3
      batch-size = 1000 # ct messages
      // Maximum amount of time before flushing change buffer to S3
      flush-timeout = 300000 # ms

      bucket = ${?AWS_S3_BUCKET}
      key-prefix = ${?AWS_S3_KEY_PREFIX}
      batch-size = ${?AWS_S3_BATCH_SIZE}
      flush-timeout = ${?AWS_S3_FLUSH_TIMEOUT}
    }

    // Timeout for requests to AWS
    timeout = 5000 #ms

    sqs.queue = ${?AWS_SQS_QUEUE_NAME}
    sns.topic = ${?AWS_SNS_TOPIC_NAME}
    timeout = ${?AWS_TIMEOUT}
    region = ${?AWS_REGION}
  }

  // Http status and control server
  control.host = 127.0.0.1
  control.host = ${?CONTROL_HOST}
  control.port = 9521
  control.port = ${?CONTROL_PORT}
}

akka {
  loggers = ["akka.event.slf4j.Slf4jLogger"]
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"
  log-dead-letters = 10
  log-dead-letters-during-shutdown = on
  loglevel = "INFO"
}
